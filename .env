# .env - Configuration for GCS-to-BigQuery Parquet Processor
TYPE="service_account"
PROJECT_ID="poised-space-456813-t0"
PRIVATE_KEY_ID="315fc454cf010e9ad16ffed87b52fc81f915527c"
PRIVATE_KEY="-----BEGIN PRIVATE KEY-----\nMIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC9RCyk3+js8GgM\njrP2ERJj5RzIcWt8CcrQ5ZC8Wf2AOc/E7SU2Fkurns7wPi5gHh9Dck3ddfcW8GsO\nxhqdDSVWOh6cPpN//ZdNRyXY30D92VSZEYc0sQR5/imtr+72R6zJd+hW2+v+M3lm\ni0YmLrK5I84OcoLM8ykKGwQAzcI06cKCD+3dN4MfpqAz+R+/SO4mnD7Pgvjvdc8O\nmgjSwHO0wDBcLaVf0DwKWLMRb+U7nyvEavyuOjseix0MdHgOZ+gZUCEnCljSy2iH\nC97DyQ/MUh7PplRCNXgInuTvcHbNY7OqT7V+cboqff1kqPMbwkqH3q+PT05KGYDC\nMIebkBlNAgMBAAECggEADDOV8TciLAw/mPemMGnfVqNRnLZ7KG71LUxvbUUb66VO\neLKVIIyFalX9n01S7g1jFJUM4gcV29u5+yTYWGGdk5aBnK8IEUtG8stwbC/QX80g\nABI6NjJnSHoTM8hQzn3GbOKXDup0p1moLO0zo4iFKxxcz1+ggRf4AFfHhUm+iGyg\n9p7FyFFYODXtZ4pbR4YxU2g6VH2kLWLCZ5q49pLYE241ZDjTkqsdC+97mt7gp6Mf\nJQ3PvlvK9pjE3w3gJ+j7N8CMPucs+ZP1W2IdQcZJMYHDleU8TRodOEBT/EJoNvh6\n/1PyJLOkuIa9D90NVq3VEalwCBRh7WxW+sfPbFSbvQKBgQD2ZjyCixqyHwU0TV3v\nAa4rN9+tPQcsKh/Efbrv+dTYbrPdXmwuKqtKnHF2B1Hax0MbyBzfpQ2koff+t/Rj\n5iMBpYjLEaAvEYiQKDOM6VKsRcFNFxmjuM/urResPkyozGFOaTxNOaxaSpFIAZBp\nJTVVDc806vDhIzYxYNsBFW6QawKBgQDEpA1SDGykzCYzhayH0XM0Q2kHUab2F76i\noQqwTwZ4S6p6tpubtuPihVH34gl36Z4950hPEwSjbf0BIrWRiNgR2iHcRFkF5klk\nnp6Y/qnVRNOGbRApZY/PYSFKtnK6G2Kd+sr4vZqSN1Sl9deShdb05sq53lR9KO6O\nRGJMSWqLJwKBgQDfV1RJz+Tmx49kFSyr1A4XRpIM6WBy+8vHw1+K3h28ALwwT8In\nJq4U9zExFvlvIWcG66CgZ2+yZs3vqLZxRs08VNtllhhoEbCDWq0/cM5rMN9CDUpE\nSOTRJzxp+B5scrVQ6bqrFTfQr/54ElLfu8ZXziifgsXMjKo2n+BjXxXU+QKBgChm\nsFnwNY4yZ5LkXwtUrD/LrdOUIiKjxEYzojeRNPoYScGGrTfz/qQ20h2E8n8Rn7KG\nR2P/u088tOO71uQ84m/gil5wKH4+z0t+D4XNpOUti+wtkkl/FWn9QdLQ44d7QurE\ncVlj01NBTQIAHbd8CistE+/WTzOb6k+Om0ejOeG3AoGAEEQh/gza9f8SY1rfon5K\nkr4DhErD1xn75Zhl5Bmw98XPj7rPoTDCgtFkfC7mJFH3jcOQ7I92kkVdi5awHexI\nGx0sNFJjD7F+I4VPeORnJrc5PSG2Ya9rCUflz5khEx+oNfCk0BEbtBDFRS0i2W+W\nzuBr/a9L1dkSd468PVpdc+I=\n-----END PRIVATE KEY-----\n"
CLIENT_EMAIL="adminaccount@poised-space-456813-t0.iam.gserviceaccount.com"
CLIENT_ID="101318438910462521713"
AUTH_URI="https://accounts.google.com/o/oauth2/auth"
TOKEN_URI="https://oauth2.googleapis.com/token"
AUTH_PROVIDER_X509_CERT_URL="https://www.googleapis.com/oauth2/v1/certs"
CLIENT_X509_CERT_URL="https://www.googleapis.com/robot/v1/metadata/x509/adminaccount%40poised-space-456813-t0.iam.gserviceaccount.com"
UNIVERSE_DOMAIN="googleapis.com"


# --- Google Cloud Project Configuration ---
GCP_PROJECT_ID="poised-space-456813-t0"
GCP_REGION="your-gcp-region"            

# --- Source GCS Configuration ---
GCS_BUCKET_NAME="miguelsiloli-housing-data" 

GCS_SUBFOLDER_PATH="linkedin/raw"

# --- Target BigQuery Configuration ---
LINKEDIN_BQ_PROJECT_ID="poised-space-456813-t0"
LINKEDIN_BQ_DATASET_ID="linkedin"
LINKEDIN_BQ_TABLE_ID="linkedin_jobs_staging"
LINKEDIN_BQ_AUGMENTED_TABLE_ID="linkedin_augmented_staging"

# Combined Full Table ID (Often more convenient in code)
# Format: project_id.dataset_id.table_id
BQ_TABLE_FULL_ID="${BQ_PROJECT_ID}.${BQ_DATASET_ID}.${BQ_TABLE_ID}"

# --- BigQuery Load Job Configuration ---
# How to handle data writing:
# WRITE_APPEND: Add rows to the table. (Default & Recommended for this use case)
# WRITE_TRUNCATE: Overwrite the table with new data (Use with caution!).
# WRITE_EMPTY: Only write if the table is empty.
BQ_WRITE_DISPOSITION="WRITE_APPEND"

# How to handle table creation:
# CREATE_IF_NEEDED: Create the table if it doesn't exist (requires schema definition or inference).
# CREATE_NEVER: Fail if the table doesn't exist. (Default & Recommended if schema is pre-defined)
BQ_CREATE_DISPOSITION="CREATE_NEVER"

# How to handle schema updates (if CREATE_IF_NEEDED or WRITE_APPEND/TRUNCATE is used):
# Example: "ALLOW_FIELD_ADDITION" to allow adding new columns. Leave empty ("") for no schema updates.
# See BigQuery docs for SchemaUpdateOptions.
BQ_SCHEMA_UPDATE_OPTIONS="" # e.g., "ALLOW_FIELD_ADDITION,ALLOW_FIELD_RELAXATION"

# --- Function Behavior ---
LOG_LEVEL="INFO" # Logging level (e.g., DEBUG, INFO, WARNING, ERROR)

PREFECT_API_KEY="pnu_6DlNEVJjrmJcEMwyaZoHt6PyGbglg42HaDxu"
PREFECT_API_URL="https://api.prefect.cloud/api/accounts/f664a625-d2c2-4256-933a-fa99ca8c3d5d/workspaces/cf4be9d7-3fbc-4f65-bb9f-b4640f5f083f"
PREFECT_API_DATABASE_CONNECTION_URL="postgresql+asyncpg://neondb_owner:npg_8DHuxKnGP7Lf@ep-rough-breeze-ab9tm9uq-pooler.eu-west-2.aws.neon.tech/neondb"
PREFECT_API_DATABASE_CONNECT_ARGS='{"sslmode": "require"}'


GEMINI_API_KEY="AIzaSyAuA48i-foDU79f8iauQsDRtl9zCafMZE8"

# .env file for LinkedIn Scraper configuration
# ============================================
# Instructions:
# 1. Fill in the required values below.
# 2. Choose ONE authentication method (Email/Password OR Cookie). Fill in the chosen one
#    and leave the other BLANK or commented out.
# 3. IMPORTANT: Add this file to your .gitignore to avoid committing credentials!

# --- LinkedIn Credentials (Choose ONE method) ---

# Option 1: Email/Password
# (Leave blank or comment out if using the cookie method)
LINKEDIN_EMAIL="miguel98oliveira@gmail.com"
LINKEDIN_PASSWORD="9121759591mM!!"

# Option 2: li_at Cookie
# (Leave blank or comment out if using the email/password method)
# How to get it: Log in to LinkedIn in your browser, open Developer Tools (F12),
# go to Application (or Storage) -> Cookies -> https://www.linkedin.com, find the 'li_at' cookie, and copy its 'Value'.
LI_AT_COOKIE=""


# --- Search Parameters ---
# Keywords for the job search (use quotes if it includes spaces or special chars like OR)
SEARCH_KEYWORDS="Data Scientist OR Data Engineer OR Machine Learning Engineer"

# Location for the job search
LOCATION="Portugal"


# --- Scraping Control ---
# Directory where the scraped HTML files will be saved
OUTPUT_DIR="linkedin_job_pages_prefect_env"

# Maximum number of job results pages to scrape
MAX_PAGES_TO_SCRAPE="100"

# Maximum time (in seconds) to wait for page elements (like search boxes, job lists) to load
PAGE_LOAD_TIMEOUT="25"

# Base delay (in seconds) between performing actions like clicking, typing, or scrolling
# Increase this if the script runs too fast and gets blocked or misses elements.
INTERACTION_DELAY="2.5"

# How many times to scroll down *within* a single job results page to load more job cards
# LinkedIn loads jobs dynamically as you scroll the list on the left.
SCROLL_PAUSES_WITHIN_PAGE="4"

# Delay (in seconds) between each individual scroll action *within* a page.
# Gives the page time to load new job cards after a scroll.
DELAY_BETWEEN_SCROLLS="3.0"